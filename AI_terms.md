# AI Terms

This file lists 20 important AI terms with short descriptions.

1. Supervised Learning — Training models on labeled input-output pairs so they learn to map inputs to known targets; commonly used for classification and regression tasks.
2. Unsupervised Learning — Finding structure in unlabeled data (e.g., clustering, dimensionality reduction) to discover patterns or segment data without explicit labels.
3. Reinforcement Learning — Learning policies through trial-and-error using a reward signal, suited for sequential decision problems like robotics and resource allocation.
4. Transfer Learning — Reusing a model or representations trained on one task to accelerate learning on a related task, reducing data and compute needs.
5. Fine-tuning — Adapting a pre-trained model on a smaller, task-specific dataset by continuing training with a low learning rate for specialization.
6. Large Language Model (LLM) — Very large transformer-based models trained on massive text corpora that can generate, summarize, and reason about language.
7. Transformer — Neural architecture that uses self-attention to model relationships across sequences, enabling strong performance in NLP and other domains.
8. Attention Mechanism — Component that weights input elements by relevance so models can focus on the most important context for each prediction.
9. Convolutional Neural Network (CNN) — Networks that apply convolutional filters to capture spatial hierarchies in images and other grid-structured data.
10. Embeddings — Dense vector representations that encode semantic meaning for words, items, or images, enabling similarity search and downstream tasks.
11. Diffusion Model — Generative approach that learns to produce data by reversing a gradual noising process, effective for high-quality image synthesis.
12. Overfitting — When a model memorizes training data including noise and fails to generalize to unseen data; a common generalization failure mode.
13. Regularization — Techniques (e.g., dropout, weight decay, augmentation) used during training to improve generalization and reduce overfitting.
14. Hyperparameter Tuning — Systematic search for optimal training/configuration parameters (learning rate, batch size, architecture choices) to maximize performance.
15. Model Drift — Decline in model effectiveness over time due to changing data distributions, requiring monitoring and retraining strategies.
16. Federated Learning — Privacy-preserving training where models are updated locally on devices and only aggregated centrally, avoiding raw data transfer.
17. Edge Inference — Running trained models on-device (phones, IoT) to reduce latency and preserve privacy, constrained by compute, memory, and power.
18. Adversarial Example — Inputs intentionally perturbed in subtle ways that cause models to make incorrect predictions, exposing robustness vulnerabilities.
19. Explainability (XAI) — Methods and tools (e.g., SHAP, LIME, saliency maps) that provide interpretable explanations for model decisions to build trust and meet regulations.
20. Prompt Engineering — Crafting and iterating input prompts to steer LLMs toward desired outputs, improving reliability and reducing unwanted behavior.

---

Generated and added to repository by Copilot.
